from torch import nn, normfrom transformers import BertModelclass BERTModel(nn.Module):    def __init__(self, num_classes, dropout=0.1):        super(BERTModel, self).__init__()        self.name = "BERT"        self.bert = BertModel.from_pretrained("bert-base-uncased")        self.dropout = nn.Dropout(dropout)        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)        self.sigmoid = nn.Sigmoid()    def forward(self, input_ids, attention_mask):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        pooled_output = outputs.pooler_output        pooled_output = self.dropout(pooled_output)        logits = self.classifier(pooled_output)        probabilities = self.sigmoid(logits)        return probabilitiesclass LSTMModel(nn.Module):    def __init__(self, vocab_size, embedding_dim, lstm_dim, output_dim, num_layers, l2_reg):        super(LSTMModel, self).__init__()        self.name = "LSTM"        self.embedding = nn.Embedding(vocab_size, embedding_dim)        self.lstm = nn.LSTM(embedding_dim, lstm_dim, num_layers=num_layers, batch_first=True)        self.fc = nn.Linear(lstm_dim, output_dim)        self.sigmoid = nn.Sigmoid()        self.l2_reg = l2_reg    def forward(self, text):        embedded = self.embedding(text)        output, (hidden, cell) = self.lstm(embedded)        output = self.fc(hidden[-1, :, :])        output = self.sigmoid(output)        l2_loss = 0        for param in self.parameters():            l2_loss += norm(param, 2)        output = output - self.l2_reg * l2_loss        return output